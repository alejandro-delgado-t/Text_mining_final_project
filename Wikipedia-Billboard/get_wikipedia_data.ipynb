{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scraping Top 100 Billboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: c:\\Users\\Enzo\\Documents\\BSE\\T2\\TEXT_MINING\\Final_Paper\\Text_mining_final_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the new working directory\n",
    "#new_path = r\"C:\\Users\\aleja\\OneDrive\\Escritorio\\Term_2\\Text_Mining\\final_project\"\n",
    "\n",
    "# Change the working directory\n",
    "#os.chdir(new_path)\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current Working Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for the year 2004...\n",
      "Scraping data for the year 2005...\n",
      "Scraping data for the year 2006...\n",
      "Scraping data for the year 2007...\n",
      "Scraping data for the year 2008...\n",
      "Scraping data for the year 2009...\n",
      "Scraping data for the year 2010...\n",
      "Scraping data for the year 2011...\n",
      "Scraping data for the year 2012...\n",
      "Scraping data for the year 2013...\n",
      "Scraping data for the year 2014...\n",
      "Scraping data for the year 2015...\n",
      "Scraping data for the year 2016...\n",
      "Scraping data for the year 2017...\n",
      "Scraping data for the year 2018...\n",
      "Scraping data for the year 2019...\n",
      "Scraping data for the year 2020...\n",
      "Scraping data for the year 2021...\n",
      "Scraping data for the year 2022...\n",
      "Scraping data for the year 2023...\n",
      "Scraping data for the year 2024...\n",
      "Data saved to billboard_hot_100_2004_2024.csv\n",
      "      year ranking               title                                artist\n",
      "0     2004       1               Yeah!  Usher featuring Lil Jon and Ludacris\n",
      "1     2004       2                Burn                                 Usher\n",
      "2     2004       3  If I Ain't Got You                           Alicia Keys\n",
      "3     2004       4           This Love                              Maroon 5\n",
      "4     2004       5    The Way You Move        OutKast featuring Sleepy Brown\n",
      "...    ...     ...                 ...                                   ...\n",
      "2087  2024      96         Bulletproof                            Nate Smith\n",
      "2088  2024      97                Fe!n  Travis Scott featuring Playboi Carti\n",
      "2089  2024      98         The Painter                          Cody Johnson\n",
      "2090  2024      99            Down Bad                          Taylor Swift\n",
      "2091  2024     100     Dance the Night                              Dua Lipa\n",
      "\n",
      "[2092 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the range of years to scrape\n",
    "years = range(2004, 2025)\n",
    "\n",
    "# List to store all data\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Scraping data for the year {year}...\")\n",
    "    \n",
    "    # Construct the Wikipedia URL for the specific year\n",
    "    url = f\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_{year}\"\n",
    "    \n",
    "    try:\n",
    "        # Request page content with UTF-8 encoding\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error if the request fails\n",
    "        response.encoding = \"utf-8\"  # Ensure UTF-8 encoding\n",
    "\n",
    "        # Parse page content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find the first wikitable on the page\n",
    "        table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "        \n",
    "        # Extract table rows\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Extract table data\n",
    "        for row in rows[1:]:  # Skip header row\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 3:  # Ensure row has the required columns\n",
    "                ranking = cols[0].text.strip()\n",
    "                title = cols[1].text.strip()\n",
    "                artist = cols[2].text.strip()\n",
    "                all_data.append([year, ranking, title, artist])\n",
    "        \n",
    "        # Sleep for 1 second to avoid overwhelming Wikipedia's servers\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape data for {year}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_data, columns=[\"Year\", \"Ranking\", \"Title\", \"Artist(s)\"])\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={\"Year\": \"year\", \"Ranking\": \"ranking\", \"Title\": \"title\", \"Artist(s)\": \"artist\"}, inplace=True)\n",
    "\n",
    "# Remove quotation marks from title column\n",
    "df[\"title\"] = df[\"title\"].str.replace('\"', '', regex=False)\n",
    "\n",
    "# Save to CSV with UTF-8 encoding\n",
    "df.to_csv(\"billboard_hot_100_2004_2024.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Data saved to billboard_hot_100_2004_2024.csv\")\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV with UTF-8 encoding\n",
    "df.to_csv(\"billboard_hot_100_2004_2024.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
