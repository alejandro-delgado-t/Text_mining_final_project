{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: C:\\Users\\aleja\\OneDrive\\Escritorio\\Term_2\\Text_Mining\\final_project\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set the new working directory\n",
    "new_path = r\"C:\\Users\\aleja\\OneDrive\\Escritorio\\Term_2\\Text_Mining\\final_project\"\n",
    "\n",
    "# Change the working directory\n",
    "os.chdir(new_path)\n",
    "\n",
    "# Verify the change\n",
    "print(\"Current Working Directory:\", os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping data for the year 1981...\n",
      "Scraping data for the year 1982...\n",
      "Scraping data for the year 1983...\n",
      "Scraping data for the year 1984...\n",
      "Scraping data for the year 1985...\n",
      "Scraping data for the year 1986...\n",
      "Scraping data for the year 1987...\n",
      "Scraping data for the year 1988...\n",
      "Scraping data for the year 1989...\n",
      "Scraping data for the year 1990...\n",
      "Scraping data for the year 1991...\n",
      "Scraping data for the year 1992...\n",
      "Scraping data for the year 1993...\n",
      "Scraping data for the year 1994...\n",
      "Scraping data for the year 1995...\n",
      "Scraping data for the year 1996...\n",
      "Scraping data for the year 1997...\n",
      "Scraping data for the year 1998...\n",
      "Scraping data for the year 1999...\n",
      "Scraping data for the year 2000...\n",
      "Scraping data for the year 2001...\n",
      "Scraping data for the year 2002...\n",
      "Scraping data for the year 2003...\n",
      "Scraping data for the year 2004...\n",
      "Scraping data for the year 2005...\n",
      "Scraping data for the year 2006...\n",
      "Scraping data for the year 2007...\n",
      "Scraping data for the year 2008...\n",
      "Scraping data for the year 2009...\n",
      "Scraping data for the year 2010...\n",
      "Scraping data for the year 2011...\n",
      "Scraping data for the year 2012...\n",
      "Scraping data for the year 2013...\n",
      "Scraping data for the year 2014...\n",
      "Scraping data for the year 2015...\n",
      "Scraping data for the year 2016...\n",
      "Scraping data for the year 2017...\n",
      "Scraping data for the year 2018...\n",
      "Scraping data for the year 2019...\n",
      "Scraping data for the year 2020...\n",
      "Scraping data for the year 2021...\n",
      "Data saved to billboard_hot_100_1981_2021.csv\n",
      "      Year Ranking                                Title  \\\n",
      "0     1981       1                   \"Bette Davis Eyes\"   \n",
      "1     1981       2                       \"Endless Love\"   \n",
      "2     1981       3                               \"Lady\"   \n",
      "3     1981       4          \"(Just Like) Starting Over\"   \n",
      "4     1981       5                      \"Jessie's Girl\"   \n",
      "...    ...     ...                                  ...   \n",
      "4077  2021      96           \"Things a Man Oughta Know\"   \n",
      "4078  2021      97              \"Throat Baby (Go Baby)\"   \n",
      "4079  2021      98                          \"Tombstone\"   \n",
      "4080  2021      99  \"Drinkin' Beer. Talkin' God. Amen.\"   \n",
      "4081  2021     100                         \"Todo de Ti\"   \n",
      "\n",
      "                                      Artist(s)  \n",
      "0                                    Kim Carnes  \n",
      "1                    Diana Ross & Lionel Richie  \n",
      "2                                  Kenny Rogers  \n",
      "3                                   John Lennon  \n",
      "4                              Rick Springfield  \n",
      "...                                         ...  \n",
      "4077                              Lainey Wilson  \n",
      "4078                                   BRS Kash  \n",
      "4079                                   Rod Wave  \n",
      "4080  Chase Rice featuring Florida Georgia Line  \n",
      "4081                             Rauw Alejandro  \n",
      "\n",
      "[4082 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the range of years to scrape\n",
    "years = range(1981, 2022)\n",
    "\n",
    "# List to store all data\n",
    "all_data = []\n",
    "\n",
    "for year in years:\n",
    "    print(f\"Scraping data for the year {year}...\")\n",
    "    \n",
    "    # Construct the Wikipedia URL for the specific year\n",
    "    url = f\"https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_{year}\"\n",
    "    \n",
    "    try:\n",
    "        # Request page content with UTF-8 encoding\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an error if the request fails\n",
    "        response.encoding = \"utf-8\"  # Ensure UTF-8 encoding\n",
    "\n",
    "        # Parse page content\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Find the first wikitable on the page\n",
    "        table = soup.find(\"table\", {\"class\": \"wikitable\"})\n",
    "        \n",
    "        # Extract table rows\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        # Extract table data\n",
    "        for row in rows[1:]:  # Skip header row\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 3:  # Ensure row has the required columns\n",
    "                ranking = cols[0].text.strip()\n",
    "                title = cols[1].text.strip()\n",
    "                artist = cols[2].text.strip()\n",
    "                all_data.append([year, ranking, title, artist])\n",
    "        \n",
    "        # Sleep for 1 second to avoid overwhelming Wikipedia's servers\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape data for {year}: {e}\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_data, columns=[\"Year\", \"Ranking\", \"Title\", \"Artist(s)\"])\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={\"Year\": \"year\", \"Ranking\": \"ranking\", \"Title\": \"title\", \"Artist(s)\": \"artist\"}, inplace=True)\n",
    "\n",
    "# Remove quotation marks from title column\n",
    "df[\"title\"] = df[\"title\"].str.replace('\"', '', regex=False)\n",
    "\n",
    "# Save to CSV with UTF-8 encoding\n",
    "df.to_csv(\"billboard_hot_100_1981_2021.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Data saved to billboard_hot_100_1981_2021.csv\")\n",
    "\n",
    "# Display DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV with UTF-8 encoding\n",
    "df.to_csv(\"billboard_hot_100_1981_2021.csv\", index=False, encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
